{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 5 - Data Crawling from Website\n",
    "\n",
    "이번 과제에서 학생들은 웹사이트에서 데이터를 수집하고 원하는 형태로 정리하는 연습을 하게될 것이다.\n",
    "\n",
    "* 제출 방법: ETL (11/20 자정까지)\n",
    "* 파일 이름: A5-학번-이름(영어로).iphynb 형식으로 (예: A5-13403-999-jiyoon.iphynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import random\n",
    "import re\n",
    "import json\n",
    "\n",
    "rand_url = \"http://www.todayhumor.co.kr/board/view.php?table=bestofbest&no=287215&s_no=287215&page=1\"\n",
    "#rand_url = \"http://www.todayhumor.co.kr/board/view.php?table=bestofbest&no=287929&s_no=287929&page=1\"\n",
    "rand_url = \"http://www.todayhumor.co.kr/board/view.php?table=bestofbest&no=287974&s_no=287974&page=1\"\n",
    "    \n",
    "def getInfos(url):\n",
    "    \n",
    "    hdr = {'User-Agent':'Mozilla/5.0'}\n",
    "    req = urllib.request.Request(url,headers=hdr)\n",
    "\n",
    "    doc = \"\"\n",
    "    with urllib.request.urlopen(req) as url:\n",
    "        doc = url.read()\n",
    "\n",
    "    soup = BeautifulSoup(doc, \"html.parser\")\n",
    "\n",
    "    ## get Writer Infomation\n",
    "    writerInfoContents = soup.find_all(\"div\", class_=\"writerInfoContents\")\n",
    "    divTags = writerInfoContents[0].find_all(\"div\")\n",
    "\n",
    "    contentID = divTags[0].text.split(' : ')[1]\n",
    "    writerName = (divTags[1].find_all(\"b\"))[0].text   # 글쓴이 닉네임\n",
    "    writerInfo_ = (divTags[1].find_all(\"span\"))[-1].text\n",
    "    writerSignInDate = re.split(':| |\\)',writerInfo_)[1]   # 글쓴이 가입 날짜\n",
    "    writerVisitingCount = re.split(':| |\\)',writerInfo_)[3]  # 방문 횟수\n",
    "    recommendCount = divTags[2].find_all(\"span\")[0].text   # 추천수\n",
    "    viewCount = divTags[3].text.split(' : ')[1]            # 조회수 \n",
    "    commentCount = re.split(' : |개',divTags[5].text)[1]   # 댓글 수 \n",
    "    BOBTime = divTags[6].text.split(' : ')[1]              # 베스트 등록 시간\n",
    "    postTIme = divTags[7].text.split(' : ')[1]             # 게시 시간\n",
    "\n",
    "    #### 게시자의 다른 글 개수 ####\n",
    "    baseURL = \"http://www.todayhumor.co.kr/board/\"\n",
    "    writerProfileURLSuffix = divTags[1].find_all(\"a\")[0][\"href\"]\n",
    "    writerProfileURL = baseURL + writerProfileURLSuffix\n",
    "    writerProfileReq = urllib.request.Request(writerProfileURL,headers=hdr)\n",
    "\n",
    "    writerProfileDoc = \"\"\n",
    "    with urllib.request.urlopen(writerProfileReq) as url:\n",
    "        writerProfileDoc = url.read()\n",
    "\n",
    "    writerProfileSoup = BeautifulSoup(writerProfileDoc, \"html.parser\")    \n",
    "    table_list = writerProfileSoup.find_all(\"table\", class_=\"table_list\")\n",
    "    normalPostCount = table_list[0].find(\"a\").text         # 유저가 쓴 글 개수\n",
    "\n",
    "    baseURL = \"http://www.todayhumor.co.kr\"\n",
    "    member_menu_box = writerProfileSoup.find_all(\"div\", class_=\"member_menu_box\")\n",
    "    writerProfileURLs = member_menu_box[0].find_all(\"a\")\n",
    "    writerProfileBestURL = baseURL + writerProfileURLs[1][\"href\"]\n",
    "    writerProfileBOBURL = baseURL + writerProfileURLs[2][\"href\"]\n",
    "\n",
    "    writerProfileReq = urllib.request.Request(writerProfileBestURL,headers=hdr)\n",
    "    writerProfileDoc = \"\"\n",
    "    with urllib.request.urlopen(writerProfileReq) as url:\n",
    "        writerProfileDoc = url.read()\n",
    "\n",
    "    writerProfileSoup = BeautifulSoup(writerProfileDoc, \"html.parser\")    \n",
    "    table_list = writerProfileSoup.find_all(\"table\", class_=\"table_list\")\n",
    "    bestPostCount = table_list[0].find(\"a\").text            # 유저가 쓴 베스트 개수\n",
    "\n",
    "    writerProfileReq = urllib.request.Request(writerProfileBOBURL,headers=hdr)\n",
    "    writerProfileDoc = \"\"\n",
    "    with urllib.request.urlopen(writerProfileReq) as url:\n",
    "        writerProfileDoc = url.read()\n",
    "\n",
    "    writerProfileSoup = BeautifulSoup(writerProfileDoc, \"html.parser\")    \n",
    "    table_list = writerProfileSoup.find_all(\"table\", class_=\"table_list\")\n",
    "    BOBPostCount = table_list[0].find(\"a\").text             # 유저가 쓴 베오베 개수\n",
    "\n",
    "    ################################\n",
    "\n",
    "    ## get Bulletin board Info\n",
    "    viewSubjectDiv = soup.find_all(\"div\", class_=\"viewSubjectDiv\")\n",
    "\n",
    "    board_icon = viewSubjectDiv[0].find_all(\"span\")[0][\"class\"][1]    # 게시판 이름 \n",
    "    title = viewSubjectDiv[0].find_all(\"div\")[0].text.strip()         # 제목\n",
    "\n",
    "    ## post option : 창작글, 펌글, 베스트 금지, 베오베 금지, 본인삭제금지, 외부펌 금지\n",
    "    contentContainer = soup.find_all(\"div\", class_=\"contentContainer\")\n",
    "    postOption = contentContainer[0].find_all(\"table\")\n",
    "\n",
    "    changjakOption = False # 창작글\n",
    "    permOption = False # 펌글\n",
    "    prohibitBestOption = False # 베스트 금지\n",
    "    prohibitBOBOption = False # 베오베 금지\n",
    "    prohibitBoninOption = False # 본인삭제 금지\n",
    "    prohibitOutsidePermOption = False # 외부펌 금지\n",
    "\n",
    "\n",
    "    if len(postOption) is 2 :  # 첫 <table>은 글 옵션 두번째 <table>은 출처. 즉 테이블이 1개이면 출처만 있고 옵션이 없음\n",
    "\n",
    "        options = postOption[0].find_all(\"li\")\n",
    "\n",
    "        for option in options : \n",
    "            optionName = option.find_all(\"div\")[1].text\n",
    "            if optionName == \"창작글\" :\n",
    "                changjakOption = True\n",
    "            elif optionName == \"펌글\" :\n",
    "                permOption = True\n",
    "            elif optionName == \"베스트금지\" :\n",
    "                prohibitBestOption = True\n",
    "            elif optionName == \"베오베금지\" :\n",
    "                prohibitBOBOption = True\n",
    "            elif optionName == \"본인삭제금지\" :\n",
    "                prohibitBoninOption = True\n",
    "            elif optionName == \"외부펌금지\" :\n",
    "                prohibitOutsidePermOption = True    \n",
    "\n",
    "    ## 본문 내용 가져오기\n",
    "    # 구해야 할것 :  이미지 개수, 동영상 유무(youtube or 움짤), 텍스트 길이, 키워드\n",
    "\n",
    "    viewContent = soup.find_all(\"div\", class_=\"viewContent\")\n",
    "    imgs = viewContent[0].find_all(\"img\") # 본문 삽입된 이미지들\n",
    "    videos = viewContent[0].find_all(\"video\")\n",
    "    youtubes = viewContent[0].find_all(\"iframe\")\n",
    "    texts = viewContent[0].find_all(\"div\")\n",
    "\n",
    "    textData = []\n",
    "    for text in texts :\n",
    "        if (text.text) :\n",
    "            textData.append(text.text)\n",
    "\n",
    "    imgCount = len(imgs)   # 이미지 카운트\n",
    "    videoCount = len(videos)   # 움짤 카운트\n",
    "    youtubeCount = len(youtubes)   # 유투브(iframe) 개수\n",
    "    textLineCount = len(textData)   # 텍스트 줄 수\n",
    "\n",
    "\n",
    "    ## 추천 시간대 \n",
    "    recommendList = re.split('  | \\xa0',soup.find_all(\"div\", id=\"ok_layer\")[0].text)\n",
    "    recommendDate = []           # 추천 시간대\n",
    "    i=1\n",
    "    while i<len(recommendList) : \n",
    "        recommendDate.append(recommendList[i])\n",
    "        i = i + 4\n",
    "\n",
    "    ## JSON 형태로 변환\n",
    "    returnJson = {recommendDate[x] : 1 for x in range(len(recommendDate))}\n",
    "    return json.dumps(returnJson,sort_keys=True,indent=4, separators=(',', ': '))\n",
    "\n",
    "recommendDate = getInfos(rand_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n    \"2016/12/07 18:20:20\": 1,\\n    \"2016/12/07 18:23:53\": 1,\\n    \"2016/12/07 18:27:33\": 1,\\n    \"2016/12/07 18:30:41\": 1,\\n    \"2016/12/07 19:07:37\": 1,\\n    \"2016/12/07 19:12:29\": 1,\\n    \"2016/12/07 19:43:56\": 1,\\n    \"2016/12/07 19:49:42\": 1,\\n    \"2016/12/07 20:25:11\": 1,\\n    \"2016/12/07 21:06:55\": 1\\n}'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommendDate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
